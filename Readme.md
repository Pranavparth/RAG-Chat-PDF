# RAG-Chat-PDF ğŸ“„ğŸ’¬

A lightweight **Retrieval-Augmented Generation (RAG)** project that lets you **upload PDFs** and ask questions about them using **LLM embeddings + Pinecone vector DB + Streamlit frontend**.

---

## âš¡ Features
- Upload PDF documents ğŸ“š
- Extract and embed text using **Llama embeddings**
- Store embeddings in **Pinecone vector DB**
- Query stored documents and get context-aware answers
- Simple **Streamlit app** for interaction

---

## ğŸ› ï¸ Tech Stack
- **Python 3.11**
- [Streamlit](https://streamlit.io/) â€“ frontend
- [Pinecone](https://www.pinecone.io/) â€“ vector database
- [PyPDF2](https://pypi.org/project/PyPDF2/) â€“ PDF parsing
- [OpenAI / Llama Embeddings](https://platform.openai.com/) â€“ embeddings

---

## ğŸ“‚ Project Structure
RAG-Chat-PDF/
â”‚â”€â”€ app.py # Streamlit frontend
â”‚â”€â”€ embed.py # PDF ingestion & embedding
â”‚â”€â”€ query.py # Query and retrieval logic
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone the repo
```bash
git clone https://github.com/YOUR_USERNAME/RAG-Chat-PDF.git
cd RAG-Chat-PDF

2ï¸âƒ£ Create virtual environment & install dependencies

python3 -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

pip install -r requirements.txt

ğŸ“Œ Usage
Upload a PDF document.
The system extracts, embeds, and stores text into Pinecone.
Enter your query in the chat UI.
Get context-aware answers instantly ğŸ¯
ğŸŒŸ Future Enhancements
Support multiple documents
Add chat history
Fine-tune embeddings
Deploy on cloud (Streamlit Cloud / HuggingFace Spaces)
